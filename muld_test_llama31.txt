2024-11-30 21:28:01.170049: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1733023681.190154 4190702 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1733023681.196457 4190702 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-30 21:28:01.217596: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO 11-30 21:28:11 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.
WARNING 11-30 21:28:11 arg_utils.py:1013] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 11-30 21:28:11 config.py:1136] Chunked prefill is enabled with max_num_batched_tokens=512.
INFO 11-30 21:28:11 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=45000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
INFO 11-30 21:28:12 selector.py:135] Using Flash Attention backend.
INFO 11-30 21:28:12 model_runner.py:1072] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 11-30 21:28:12 weight_utils.py:243] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.30it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.76it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.50it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.27it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.35it/s]

INFO 11-30 21:28:16 model_runner.py:1077] Loading model weights took 14.9888 GB
INFO 11-30 21:28:16 worker.py:232] Memory profiling results: total_gpu_memory=23.67GiB initial_memory_usage=15.24GiB peak_torch_memory=16.16GiB memory_usage_post_profile=15.28GiB non_torch_memory=0.28GiB kv_cache_size=6.04GiB gpu_memory_utilization=0.95
INFO 11-30 21:28:17 gpu_executor.py:113] # GPU blocks: 3094, # CPU blocks: 2048
INFO 11-30 21:28:17 gpu_executor.py:117] Maximum concurrency for 45000 tokens per request: 1.10x
INFO 11-30 21:28:20 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 11-30 21:28:20 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 11-30 21:28:38 model_runner.py:1518] Graph capturing finished in 18 secs, took 0.26 GiB
86
Question 1 in progress...
this is length of string_tokenized: 22408
this is the division: 0.5602
prediction villain
target: ['Hero']
Question 2 in progress...
this is length of string_tokenized: 27719
this is the division: 0.692975
prediction villain
target: ['Villain/Antagonist']
Question 3 in progress...
this is length of string_tokenized: 27719
this is the division: 0.692975
prediction hero
target: ['Hero']
Question 4 in progress...
this is length of string_tokenized: 27467
this is the division: 0.686675
prediction hero
target: ['Hero']
Question 5 in progress...
this is length of string_tokenized: 27467
this is the division: 0.686675
prediction villain
target: ['Villain/Antagonist']
Question 6 in progress...
this is length of string_tokenized: 27467
this is the division: 0.686675
prediction hero
target: ['Hero']
Question 7 in progress...
this is length of string_tokenized: 27467
this is the division: 0.686675
prediction hero
target: ['Hero']
Question 8 in progress...
this is length of string_tokenized: 27467
this is the division: 0.686675
prediction villain
target: ['Villain/Antagonist']
Question 9 in progress...
this is length of string_tokenized: 20074
this is the division: 0.50185
prediction villain
target: ['Villain/Antagonist']
Question 10 in progress...
this is length of string_tokenized: 20074
this is the division: 0.50185
prediction villain
target: ['Villain/Antagonist']
Question 11 in progress...
this is length of string_tokenized: 20074
this is the division: 0.50185
prediction hero
target: ['Hero']
Question 12 in progress...
this is length of string_tokenized: 28851
this is the division: 0.721275
prediction villain
target: ['Villain/Antagonist']
Question 13 in progress...
this is length of string_tokenized: 28851
this is the division: 0.721275
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/tmalik6/cs412/LD/llm_zero_shot_muld.py", line 231, in <module>
[rank0]:     main()
[rank0]:   File "/home/tmalik6/cs412/LD/llm_zero_shot_muld.py", line 217, in main
[rank0]:     ans_full, predicts_full, predicts = predict(test_source, llm, 40000, test_class)
[rank0]:   File "/home/tmalik6/cs412/LD/llm_zero_shot_muld.py", line 163, in predict
[rank0]:     text = generate(sp, up, inst, llm) #this part is the problem
[rank0]:   File "/home/tmalik6/cs412/LD/llm_zero_shot_muld.py", line 60, in generate
[rank0]:     output = llm.generate(request,params, use_tqdm = False) # trying use_tqdm true for the first time, dunno how it will work
[rank0]:   File "/home/tmalik6/miniconda3/envs/ld/lib/python3.10/site-packages/vllm/utils.py", line 1063, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/home/tmalik6/miniconda3/envs/ld/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 406, in generate
[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)
[rank0]:   File "/home/tmalik6/miniconda3/envs/ld/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 942, in _run_engine
[rank0]:     step_outputs = self.llm_engine.step()
[rank0]:   File "/home/tmalik6/miniconda3/envs/ld/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 1407, in step
[rank0]:     ) = self.scheduler[virtual_engine].schedule()
[rank0]:   File "/home/tmalik6/miniconda3/envs/ld/lib/python3.10/site-packages/vllm/core/scheduler.py", line 1219, in schedule
[rank0]:     scheduler_outputs: SchedulerOutputs = self._schedule()
[rank0]:   File "/home/tmalik6/miniconda3/envs/ld/lib/python3.10/site-packages/vllm/core/scheduler.py", line 1176, in _schedule
[rank0]:     return self._schedule_chunked_prefill()
[rank0]:   File "/home/tmalik6/miniconda3/envs/ld/lib/python3.10/site-packages/vllm/core/scheduler.py", line 1112, in _schedule_chunked_prefill
[rank0]:     running_scheduled = self._schedule_running(budget,
[rank0]:   File "/home/tmalik6/miniconda3/envs/ld/lib/python3.10/site-packages/vllm/core/scheduler.py", line 541, in _schedule_running
[rank0]:     assert len(self._async_stopped) == 0
[rank0]: AssertionError
[rank0]:[W1130 21:32:07.974334087 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
